# Berkeley CS182/282A: Deep Neural Networks

This repository contains my course work for **CS182/282A ‚Äì Deep Neural Networks** at **UC Berkeley**.

UC Berkeley's CS182/282A offers a rigorous deep dive into the mathematical and practical foundations of deep learning. The curriculum spans from advanced optimization and scaling theory (muP, MuON, and implicit regularization) to architectures including CNNs, GNNs, Transformers, and State-Space Models (SSMs). It culminates in modern LLM frontiers, covering Parameter-Efficient Fine-Tuning (LoRA), In-context Learning, and Generative paradigms (Diffusion), providing a comprehensive toolkit for understanding and scaling frontier AI models.

Course website: https://berkeley-cs182.github.io/fa25/

---

## üìö Contents

### üìù Notes
- Lecture notes and personal summaries
- Mathematical derivations and conceptual explanations
- Additional insights and references beyond the lectures

### üß™ Assignments
- Programming assignments and experiments
- Implementations of core deep learning algorithms
- Analysis of results and observations

---

## üß† Topics Covered

The curriculum focuses on the theoretical and practical aspects of deep learning. Key highlights include:
- **Optimization Theory**: Beyond basic SGD‚Äîimplicit regularization, muP, and MuON.
- **Architectures**: From ResNets and GNNs to the latest in SSMs and Transformers.
- **Modern LLM Techniques**: PEFT (LoRA), In-context learning, Meta-learning, and Post-training.
- **Generative AI**: VAEs, Diffusion Models.

---

## ‚ö†Ô∏è Academic Integrity

This repository is intended **for personal learning and reference only**.
Please do not copy or submit any part of this work as your own coursework.

---

## üìñ References

- CS182/282A: Deep Neural Networks (UC Berkeley)
- Course lecture slides and official materials
- Relevant research papers and textbooks
